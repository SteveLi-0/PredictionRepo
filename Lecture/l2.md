## Transformer
### Motion transformer
Detr + Transformer
### MTR Encoder
Input represenation
Polyline encoder 可以是什么？

Vectorized represenation + PointNet(MLP + MaxPooling)

如果有目标被遮挡，那么如何处理？

padding mask

PointNet Advantage?

low computation consuming

PointNet Advantage?

maxpooling 无法建模时间信息

cnn gru 可以建模时间信息

position embedding 
input: x y t
(gru better)

scene context encoding with local transformer encoder

做法1： 所有结点concat + self attention
做法2： hivt： cross attention
做法3： global graph mask

全局结点 占用显存
局部结构 更加重要

self attention 的 qkv是什么？

layer 0
concate （agent embedding + map embedding）

layer 1

query: node embedding + Positional embedding （agent A）(from layer 0)
key： node embedding （距离 agent A 最近的结点）(from layer 0)
value： node embedding （距离 agent A 最近的结点）(from layer 0)

local graph的local是什么含义？

当前时间
knn for agents and polylines
interaction redius = 50m 

map不包含时间信息
polyline之间的距离（某个点距离、最近距离）

太多的neighbour会影响推理结果 局部结构 更加重要

Dense Future Prediction
input: transformer encoder 的输出 s_1:T(trajectory) = mlp(agent embedding) 
transformer encoder 已经输出了single modal的轨迹相当于未来的信息，增加了transformer的输入
模型监督的目标从一个变成了整个场景
对未来轨迹做l1 loss监督
为decoder提供场景一致性信息

Dynamic Map Collection
根据输出的轨迹，收集距离轨迹最近的 top-L polyline


### MTR Decoder
anchor-based
anchors - prior knowledges : static intention point
k means 聚类 64个anchor  
缺点：没有考虑地图信息，anchor可能会超过boundary mAP表现不好

(detach? replay)
how to model intention query?
(64, 2) (xy)
sine + mlp == query embedding

why introduce this variable into the network?
anchor query 的效果是什么？ 让网络更加容易收敛
每个anchor 对应一个mode 降低了网络的不确定性 加速收敛

Local movement refinement
dynamic motion query

how to improve (refine) rough intention? Dynamic searching
iteratively gathering fine grained trajectory features

每一层都把Context feature 输入到decoder 做 cross attention
query: query content after mhsa + add norm + dynamic searching query(pe)
key: context feature + pe
value: context feature

为什么加pe？ 参考resnet 保留meta information

为什么用 concat ？ 不是 +
cat可以解偶 context info 和 pe info
作用不大

alpha （Map） 体现局部地图信息

context feature: agent (past + furture) + map

self attention 
multi mode imformentation interation 
diffuse

GMM 
6：高斯混合模型的参数
mu x,mu y, sigma x, sigma y, rho, 
pk -- 每个 mode 的概率


### Loss
L1 loss
Gaussian regression negative log likelihood
winner-take-all

### interaction com

marginal
每个agent single modal
joint
每个场景一个mode

## Joint feature prediction